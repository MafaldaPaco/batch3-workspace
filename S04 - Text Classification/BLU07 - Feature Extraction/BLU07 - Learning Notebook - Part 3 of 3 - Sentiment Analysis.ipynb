{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# NLTK imports\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/imdb_sentiment.csv')\n",
    "\n",
    "# Get the text\n",
    "docs = df['text']\n",
    "\n",
    "# Split in train and validation\n",
    "train_df, validation_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all of this in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Aldolpho (Steve Buscemi), an aspiring film mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>An unfunny, unworthy picture which is an undes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>A failure. The movie was just not good. It has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I saw this movie Sunday afternoon. I absolutel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Disney goes to the well one too many times as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Just PPV'd this. I don't want to waste too muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I am a guy, who loves guy movies... I was look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Positive</td>\n",
       "      <td>The opening flourishes left me purring with de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Although in some aspects Seven Pounds is solid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I don't want to go off on a rant here, but.......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I am a fan of the previous Best of the Best fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I can't even believe that this show lasted as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Kudos to Cesar Montano for reviving the Cebuan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Positive</td>\n",
       "      <td>this is a great movie. i like it where ning cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Positive</td>\n",
       "      <td>...intimate and specific. Yes, a bit of a cind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Astounding.....This may have been A poor attem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I have to admit, I don't remember much about t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I am a relative latecomer to the transcendent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Many of the classic films of the late '60s hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Positive</td>\n",
       "      <td>What a movie! I never imagined Richard Attenbo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Positive</td>\n",
       "      <td>***SPOILERS*** ***SPOILERS*** Packed with memo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I saw this film at the Santa Barbara Film Fest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Negative</td>\n",
       "      <td>This movie is lame and not funny at all. The p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Negative</td>\n",
       "      <td>What \"Noise\" fails to do is get us to understa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I saw it in a posh movie theater where the aud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I went into this movie hoping for the best. I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I found this episode to be one of funniest I'v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Positive</td>\n",
       "      <td>This movie was so great! I am a teenager, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Negative</td>\n",
       "      <td>This movie was packed pull of endless surprise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Negative</td>\n",
       "      <td>This thing is horrible. The Ben Affleck charac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Bradford Dillman plays a scientist who wakes u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>Positive</td>\n",
       "      <td>i totally disagree.i thought that this was a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>Positive</td>\n",
       "      <td>This stylistically sophisticated visual game p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Kevin Spacey is very talented, but unfortunate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>Positive</td>\n",
       "      <td>As the celebration of Christmas has evolved th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Although nothing can compare to Vampires Vs. Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Quite simply the best reality show ever made. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>Negative</td>\n",
       "      <td>The 3-D featured in \"The Man Who Wasn't There\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4978</th>\n",
       "      <td>Negative</td>\n",
       "      <td>While the acting and directing could be argued...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I have come out of several years of lurking on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Perhaps more than many films, this one is not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I'll be blunt and to the point. This film is n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>Negative</td>\n",
       "      <td>At least I was able to enjoy mocking the movie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like Final Fantasy brought CG to a whole ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>Negative</td>\n",
       "      <td>This movie is without a doubt the worst horror...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>Negative</td>\n",
       "      <td>save your money. i have been a fan of fullmoon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Peter Ustinov plays an embezzler who is just g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>Negative</td>\n",
       "      <td>After a promising first 25 minutes that makes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>Positive</td>\n",
       "      <td>The Three Stooges has always been some of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>Negative</td>\n",
       "      <td>A wonderful television mini-series completely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Don't get me wrong, this is a terrible, cliché...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Disney has now made straight-to-video sequels ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>Positive</td>\n",
       "      <td>If you delete the first twenty minutes or so o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>Positive</td>\n",
       "      <td>To all the haters out there: condemning a TV s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>Positive</td>\n",
       "      <td>For anyone who wishes to get an impression of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Enchanting, romantic, innovative, and funny. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Imagine the scenario - you are at the movie th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Dark Rising is your typical bad, obviously qui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Well, what can I say, this movie really got to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Negative</td>\n",
       "      <td>This was a truly bad film. The character \"Cole...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                               text\n",
       "0     Negative  Aldolpho (Steve Buscemi), an aspiring film mak...\n",
       "1     Negative  An unfunny, unworthy picture which is an undes...\n",
       "2     Negative  A failure. The movie was just not good. It has...\n",
       "3     Positive  I saw this movie Sunday afternoon. I absolutel...\n",
       "4     Negative  Disney goes to the well one too many times as ...\n",
       "5     Negative  Just PPV'd this. I don't want to waste too muc...\n",
       "6     Negative  I am a guy, who loves guy movies... I was look...\n",
       "7     Positive  The opening flourishes left me purring with de...\n",
       "8     Negative  Although in some aspects Seven Pounds is solid...\n",
       "9     Negative  I don't want to go off on a rant here, but.......\n",
       "10    Negative  I am a fan of the previous Best of the Best fi...\n",
       "11    Negative  I can't even believe that this show lasted as ...\n",
       "12    Positive  Kudos to Cesar Montano for reviving the Cebuan...\n",
       "13    Positive  this is a great movie. i like it where ning cl...\n",
       "14    Positive  ...intimate and specific. Yes, a bit of a cind...\n",
       "15    Positive  Astounding.....This may have been A poor attem...\n",
       "16    Negative  I have to admit, I don't remember much about t...\n",
       "17    Negative  I am a relative latecomer to the transcendent ...\n",
       "18    Positive  Many of the classic films of the late '60s hav...\n",
       "19    Positive  What a movie! I never imagined Richard Attenbo...\n",
       "20    Positive  ***SPOILERS*** ***SPOILERS*** Packed with memo...\n",
       "21    Positive  I saw this film at the Santa Barbara Film Fest...\n",
       "22    Negative  This movie is lame and not funny at all. The p...\n",
       "23    Negative  What \"Noise\" fails to do is get us to understa...\n",
       "24    Positive  I saw it in a posh movie theater where the aud...\n",
       "25    Negative  I went into this movie hoping for the best. I ...\n",
       "26    Positive  I found this episode to be one of funniest I'v...\n",
       "27    Positive  This movie was so great! I am a teenager, and ...\n",
       "28    Negative  This movie was packed pull of endless surprise...\n",
       "29    Negative  This thing is horrible. The Ben Affleck charac...\n",
       "...        ...                                                ...\n",
       "4970  Negative  Bradford Dillman plays a scientist who wakes u...\n",
       "4971  Positive  i totally disagree.i thought that this was a g...\n",
       "4972  Positive  This stylistically sophisticated visual game p...\n",
       "4973  Negative  Kevin Spacey is very talented, but unfortunate...\n",
       "4974  Positive  As the celebration of Christmas has evolved th...\n",
       "4975  Negative  Although nothing can compare to Vampires Vs. Z...\n",
       "4976  Positive  Quite simply the best reality show ever made. ...\n",
       "4977  Negative  The 3-D featured in \"The Man Who Wasn't There\"...\n",
       "4978  Negative  While the acting and directing could be argued...\n",
       "4979  Negative  I have come out of several years of lurking on...\n",
       "4980  Positive  Perhaps more than many films, this one is not ...\n",
       "4981  Negative  I'll be blunt and to the point. This film is n...\n",
       "4982  Negative  At least I was able to enjoy mocking the movie...\n",
       "4983  Positive  Just like Final Fantasy brought CG to a whole ...\n",
       "4984  Negative  This movie is without a doubt the worst horror...\n",
       "4985  Negative  save your money. i have been a fan of fullmoon...\n",
       "4986  Positive  Peter Ustinov plays an embezzler who is just g...\n",
       "4987  Negative  After a promising first 25 minutes that makes ...\n",
       "4988  Positive  The Three Stooges has always been some of the ...\n",
       "4989  Negative  A wonderful television mini-series completely ...\n",
       "4990  Negative  Don't get me wrong, this is a terrible, cliché...\n",
       "4991  Negative  Disney has now made straight-to-video sequels ...\n",
       "4992  Positive  If you delete the first twenty minutes or so o...\n",
       "4993  Positive  To all the haters out there: condemning a TV s...\n",
       "4994  Positive  For anyone who wishes to get an impression of ...\n",
       "4995  Positive  Enchanting, romantic, innovative, and funny. T...\n",
       "4996  Negative  Imagine the scenario - you are at the movie th...\n",
       "4997  Negative  Dark Rising is your typical bad, obviously qui...\n",
       "4998  Positive  Well, what can I say, this movie really got to...\n",
       "4999  Negative  This was a truly bad film. The character \"Cole...\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part I of this BLU you learned about _preprocessing_ your text data into something easier for the machine later to process and vectorize. In practice, we can create a class that transforms our data into those _easier to process_ strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to implement sentence cleaning\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, regex_list,\n",
    "                 lower=True, remove_punct=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.regex_list = regex_list\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        X = list(map(self._clean_sentence, X))\n",
    "        return X\n",
    "    \n",
    "    def _clean_sentence(self, sentence):\n",
    "        \n",
    "        # Replace given regexes\n",
    "        for regex in self.regex_list:\n",
    "            sentence = re.sub(regex[0], regex[1], sentence)\n",
    "            \n",
    "        # lowercase\n",
    "        if self.lower:\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "        # Split sentence into list of words\n",
    "        words = self.tokenizer.tokenize(sentence)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            words = list(filter(lambda x: x not in string.punctuation, words))\n",
    "\n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            words = map(self.stemmer.stem, words)\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentence = \" \".join(words)\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created a class that has a `transform()` method that will apply the method `_clean_sentence()` to every sentence of its input `X`. Note that you can choose the tokenizer and the stemmer as inputs of this class - you can choose which ones you prefer to use. You can also give the class a list of tuples that are regexes that you want to substitute for something in your sentences.\n",
    "\n",
    "Let's use the same tokenizer, stemmer and html regex that we were using before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer and a stemmer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "regex_list = [(\"<[^>]*>\", \"\")\n",
    "             ]\n",
    "\n",
    "cleaner = TextCleanerTransformer(tokenizer, stemmer, regex_list)\n",
    "docs = cleaner.transform(train_df.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an output example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a film about life the triumph over advers and the wonder of the human spirit i defi anyon not to shed a tear by the end of the movi this is more than just a tear jerker its an engag thought provok drama with excel perform from all the cast but especi derek luke and denzel washington 7 year on i m amaz that luke is still a virtual unknown and washington only direct one other film nevertheless apart from a slow build up the stori of this foster child s trial and tribul and how it still affect him in adulthood is the sort of movi that stay with you long after you have seen it like mani fox searchlight pictur this was more of a sleeper hit and didn t get the mass critic acclaim it deserv the scene where antwon final meet his mother sum up the movi for me there were so mani way that could have been done and it could have been all schmaltzi or it could have been unrealist but washington struck exact the right tone his mother never said a word and could only shed a tear while antown ask simpli why her overwhelm guilt prevent her from say anyth what could she say to defend herself one of the most move cinemat scene i have seen'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now onto what we learned in Part II."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imagine, there are many implementations in the internet of Bag of Words and TF-IDF. We're going to use scikit-learn from now on to compute the feature representations learnt in this BLU.\n",
    "\n",
    "Our BoW representations, for instance, can be done with scikit's [CountVectorizer()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). We still haven't removed stopwords, as you may have noticed. However, removing stopwords is easy with `CountVectorizer()` - just pass the parameter `stop_words` and assign it to `'english'`! This will remove all english stopwords from your corpus. You can also give this parameter a list of strings, if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small sample of the vocabulary: ['watch', 'dvd', 'movi', 'come', 'excel', 'commentari', 'track', 'english', 'cambodia', 'subtitl', 'say', 'charact', 'speak', 'thai', 'violent', 'evil', 'man', 'rais', 'boy', 'killer']\n",
      "\n",
      "Number of distinct words: 23740\n"
     ]
    }
   ],
   "source": [
    "vectorizer.fit(docs)\n",
    "\n",
    "# Looking at a small sample of the vocabulary:\n",
    "vocabulary = list(vectorizer.vocabulary_.keys())\n",
    "print(\"Small sample of the vocabulary:\", vocabulary[0:20])\n",
    "\n",
    "# Number of words in the vocabulary\n",
    "print(\"\\nNumber of distinct words:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a random sample sentence, for example sentence 12, we can visualize our bag of words representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the kind of movi which show the pauciti of french cinema when it come to make thriller the director s desir to sound american is so glare that you will not be fool a minut unless you have not seen a serial killer movi sinc peep tom \". two male cop or one and a half more like as you will see ), horribl murder a plot more complic than complex charl berl is not lucki with the genr see the astoundl dumb l inconnu de strasbourg a coupl of year ago ). the scene with his pregnant wife which are suppos to be a counterpart for the otherwis noir atmospher of the rest of the plot are among the worst ever film add a steami love scene between them and a gori autopsi to get a pg 12 and thus to attract the huge adolesc audienc a violent and absurd conclus follow by a silent epilogu who could make a nice commerci for the côte d azur it s realli the silenc of the lame \n",
      "\n",
      "12 :  1\n",
      "absurd :  1\n",
      "add :  1\n",
      "adolesc :  1\n",
      "ago :  1\n",
      "american :  1\n",
      "astoundl :  1\n",
      "atmospher :  1\n",
      "attract :  1\n",
      "audienc :  1\n",
      "autopsi :  1\n",
      "azur :  1\n",
      "berl :  1\n",
      "charl :  1\n",
      "cinema :  1\n",
      "come :  1\n",
      "commerci :  1\n",
      "complex :  1\n",
      "complic :  1\n",
      "conclus :  1\n",
      "cop :  1\n",
      "counterpart :  1\n",
      "coupl :  1\n",
      "côte :  1\n",
      "desir :  1\n",
      "director :  1\n",
      "dumb :  1\n",
      "epilogu :  1\n",
      "film :  1\n",
      "follow :  1\n",
      "fool :  1\n",
      "french :  1\n",
      "genr :  1\n",
      "glare :  1\n",
      "gori :  1\n",
      "half :  1\n",
      "horribl :  1\n",
      "huge :  1\n",
      "inconnu :  1\n",
      "killer :  1\n",
      "kind :  1\n",
      "lame :  1\n",
      "like :  1\n",
      "love :  1\n",
      "lucki :  1\n",
      "make :  2\n",
      "male :  1\n",
      "minut :  1\n",
      "movi :  2\n",
      "murder :  1\n",
      "nice :  1\n",
      "noir :  1\n",
      "otherwis :  1\n",
      "pauciti :  1\n",
      "peep :  1\n",
      "pg :  1\n",
      "plot :  2\n",
      "pregnant :  1\n",
      "realli :  1\n",
      "rest :  1\n",
      "scene :  2\n",
      "seen :  1\n",
      "serial :  1\n",
      "silenc :  1\n",
      "silent :  1\n",
      "sinc :  1\n",
      "sound :  1\n",
      "steami :  1\n",
      "strasbourg :  1\n",
      "suppos :  1\n",
      "thriller :  1\n",
      "tom :  1\n",
      "unless :  1\n",
      "violent :  1\n",
      "wife :  1\n",
      "worst :  1\n",
      "year :  1\n"
     ]
    }
   ],
   "source": [
    "sentence = docs[12:13]\n",
    "print(sentence[0], '\\n')\n",
    "\n",
    "# Tranform sentence into bag of words representation\n",
    "word_count_sentence = vectorizer.transform(sentence)\n",
    "\n",
    "# Find the indexes of the words which appear in the sentence\n",
    "_, columns = word_count_sentence.nonzero()\n",
    "\n",
    "# Get the inverse map to map vector indexes to words\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "inv_map = {v: k for k, v in vocabulary.items()}\n",
    "\n",
    "# Extract the corresponding word and count\n",
    "counts = [(inv_map[i], word_count_sentence[0, i]) for i in columns]\n",
    "\n",
    "for word, count in counts:\n",
    "    print(word, \": \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the word counts (Bag of Words representation) for every sentence by calling the transform method. This returns a sparse matrix where the rows represent the samples and the columns the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 23740)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_matrix = vectorizer.transform(df['text'].values)\n",
    "word_count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do TF-IDF, that can be done on top of our matrix of word counts with [TfidfTransformer()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(word_count_matrix)\n",
    "\n",
    "word_term_frequency_matrix = tfidf.transform(word_count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned in Part I, we can not only get features that correspond to each word in our vocabulary (Unigram) but also can get features for any number of N-grams. We can easily get all the possibilities in a chosen range of N by using `CountVectorizer()` parameter `ngram_range`. This parameter receives a tuple `(min_n, max_n)` - if we wanted to extract all unigrams and bigrams in a corpus we would pass to the model `ngram_range=(1,2)`.\n",
    "\n",
    "Let's get all unigram, bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 773317)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_123_grams = CountVectorizer(stop_words= 'english', ngram_range=(1,3))\n",
    "vectorizer_123_grams.fit(docs)\n",
    "word_count_matrix = vectorizer_123_grams.transform(df['text'].values)\n",
    "word_count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we end up with a much bigger feature space - which can be quite computationally expensive to our model! Fortunately, there are some parameters in `CountVectorizer()` that we can use to reduce our feature space while keeping as much informative representations as possible:\n",
    "\n",
    "- `max_features` - receives an `int` that will be the size of the feature space of the model. The N features chosen will be the ones with higher term frequency across the corpus.\n",
    "- `min_df` - the minimum document frequency a n-gram can have to be considered. Often called *cut-off*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the sentiment of the movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use all that we learned and create a model that predicts if a review is positive or negative. In NLP, we call this task _sentiment analysis_.\n",
    "\n",
    "<img src=\"./media/dwight.jpg\" width=\"500\">\n",
    "\n",
    "Since there are several things that we need to do sequentially to our data, we can create a [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Pipelines allow us to easily compose transformations and classifiers.\n",
    "\n",
    "The main advantage of pipelines is that the pipeline exposes the fit and predict functions, these automatically call the transformations on the data and the classifier, keeping the transformations coherent between train and test data.\n",
    "\n",
    "We will use Scikit's implementation of Naive Bayes as our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "text_clf = Pipeline([('stemm', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(stop_words='english')),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final piece that is missing is converting the character labels into numeric labels through the Scikit [Label Encoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['sentiment'].values)\n",
    "\n",
    "train_df['sentiment'] = le.transform(train_df['sentiment'].values)\n",
    "validation_df['sentiment'] = le.transform(validation_df['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('stemm',\n",
       "                 <__main__.TextCleanerTransformer object at 0x0000021325B3C470>),\n",
       "                ('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use bigrams as well in our model, along with unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.826"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the pipeline\n",
    "text_clf = Pipeline([('stemm', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, our performance on the validation set got worse! This is an example of when it can be hurtful to your model to remove stopwords, as we warned you in Part II. If you take a look at stopwords list, words like \"no\" are part of it. This can be crucial to our bigram representation since if we remove these words, relevant bigrams will not appear in our feature space (ex.: \"no fun\").\n",
    "\n",
    "Let's remove the stop_words parameter from the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.837"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the pipeline\n",
    "text_clf = Pipeline([('stemm', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a better performance, but still not as good as in our unigram model. This can be due to our feature space having too many dimensions, which can hurt model performance. Fortunately, we learned that `CountVectorizer()` has parameters that help to reduce our feature dimensionality.\n",
    "\n",
    "You should try and see if you can get higher accuracy by reducing dimensionality with `max_features` and `min_df` (Spoiler alert: you will!). You can then also try to use trigrams along with the unigrams and bigrams you already have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out different classifiers and play around with `CountVectorizer()` and `TfidfTransformer()` parameters to see if you can get better scores!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
